# ğŸš€ HÆ°á»›ng dáº«n cháº¡y Open Vocabulary Detection

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+
- **CUDA**: 11.0+ (khuyáº¿n nghá»‹)
- **RAM**: Tá»‘i thiá»ƒu 8GB, khuyáº¿n nghá»‹ 16GB+
- **GPU**: Tá»‘i thiá»ƒu 6GB VRAM

## âš¡ CÃ i Ä‘áº·t nhanh

### 1. Clone vÃ  cÃ i Ä‘áº·t dependencies
```bash
git clone <your-repo>
cd open_vocab_detection
pip install -r requirements.txt
```

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u
```bash
# Táº¡o thÆ° má»¥c dá»¯ liá»‡u
mkdir -p data/flickr30k_images
mkdir -p data/Annotations
mkdir -p data/Sentences

# Copy dá»¯ liá»‡u Flickr30k vÃ o cÃ¡c thÆ° má»¥c tÆ°Æ¡ng á»©ng
# - flickr30k_images/: Chá»©a cÃ¡c file áº£nh .jpg
# - Annotations/: Chá»©a cÃ¡c file annotation .xml  
# - Sentences/: Chá»©a cÃ¡c file cÃ¢u mÃ´ táº£ .txt
```

## ğŸ¯ CÃ¡c cÃ¡ch cháº¡y

### 1. **Test nhanh (Khuyáº¿n nghá»‹ Ä‘áº§u tiÃªn)**
```bash
python scripts/quick_test.py
```
- Kiá»ƒm tra model cÃ³ táº¡o Ä‘Æ°á»£c khÃ´ng
- Test forward pass
- Kiá»ƒm tra memory usage
- **Thá»i gian**: ~2 phÃºt

### 2. **Training vá»›i model tá»‘i Æ°u (Khuyáº¿n nghá»‹)**
```bash
python scripts/train_optimized.py
```
- Model nháº¹, training nhanh
- Batch size 24, image size 256x256
- Mixed precision, gradient clipping
- **Thá»i gian**: ~2-3 giá» (20 epochs)

### 3. **Training vá»›i model gá»‘c (So sÃ¡nh)**
```bash
python scripts/train.py
```
- Model náº·ng hÆ¡n, training cháº­m hÆ¡n
- Batch size 16, image size 384x384
- **Thá»i gian**: ~6-8 giá» (30 epochs)

### 4. **So sÃ¡nh hiá»‡u suáº¥t**
```bash
python scripts/compare_models.py
```
- So sÃ¡nh model cÅ© vs má»›i
- Äo memory usage, speed, parameters
- **Thá»i gian**: ~5 phÃºt

### 5. **Evaluation**
```bash
python scripts/evaluate.py
```
- ÄÃ¡nh giÃ¡ model Ä‘Ã£ train
- TÃ­nh mAP, precision, recall
- **Thá»i gian**: ~30 phÃºt

## ğŸ“Š Monitoring

### TensorBoard
```bash
tensorboard --logdir lightning_logs
```
Truy cáº­p: http://localhost:6006

### Key metrics:
- `train/total_loss`: Loss training (nÃªn giáº£m)
- `val/mAP`: Mean Average Precision (metric chÃ­nh)
- `val/total_loss`: Loss validation (nÃªn gáº§n train loss)
- GPU memory usage

## ğŸ”§ Tuning cho há»‡ thá»‘ng yáº¿u

### Náº¿u GPU < 6GB VRAM:
```bash
# Chá»‰nh sá»­a trong train_optimized.py
batch_size = 8          # Giáº£m tá»« 24
img_size = (224, 224)   # Giáº£m tá»« 256x256
accumulate_grad_batches = 2  # TÄƒng tá»« 1
```

### Náº¿u RAM < 8GB:
```bash
# Chá»‰nh sá»­a trong train_optimized.py
num_workers = 2         # Giáº£m tá»« 8
batch_size = 4          # Giáº£m tá»« 24
```

### Náº¿u muá»‘n training nhanh hÆ¡n:
```bash
# Chá»‰nh sá»­a trong train_optimized.py
max_epochs = 10         # Giáº£m tá»« 20
limit_val_batches = 0.2 # Giáº£m tá»« 0.5
```

## ğŸ› Troubleshooting

### Lá»—i "CUDA out of memory":
```bash
# Giáº£m batch size
batch_size = 4
# Hoáº·c tÄƒng gradient accumulation
accumulate_grad_batches = 4
```

### Lá»—i "Module not found":
```bash
# CÃ i Ä‘áº·t láº¡i dependencies
pip install -r requirements.txt
# Hoáº·c cÃ i Ä‘áº·t thá»§ cÃ´ng
pip install torch torchvision pytorch-lightning transformers
```

### Training quÃ¡ cháº­m:
```bash
# Kiá»ƒm tra GPU usage
nvidia-smi
# Giáº£m image size
img_size = (224, 224)
# TÄƒng batch size náº¿u cÃ³ Ä‘á»§ memory
batch_size = 32
```

### Káº¿t quáº£ kÃ©m:
```bash
# TÄƒng learning rate
lr = 5e-4
# TÄƒng epochs
max_epochs = 30
# TÄƒng similarity loss weight
lambda_sim = 0.5
```

## ğŸ“ Cáº¥u trÃºc output

```
lightning_logs/
â”œâ”€â”€ open_vocab_optimized/     # Logs tá»« train_optimized.py
â”‚   â”œâ”€â”€ version_0/
â”‚   â”‚   â”œâ”€â”€ events.out.tfevents.*
â”‚   â”‚   â””â”€â”€ hparams.yaml
â”‚   â””â”€â”€ checkpoints/          # Model checkpoints
â”‚       â”œâ”€â”€ last.ckpt
â”‚       â””â”€â”€ epoch=XX-map=X.XXX.ckpt
â””â”€â”€ open_vocab_detection/     # Logs tá»« train.py
```

## ğŸ‰ Káº¿t quáº£ mong Ä‘á»£i

### Model tá»‘i Æ°u (train_optimized.py):
- **Parameters**: ~28M
- **Memory**: ~3GB VRAM
- **Speed**: ~100 samples/s
- **mAP**: 0.15-0.25 (tÃ¹y dataset)

### Model gá»‘c (train.py):
- **Parameters**: ~88M  
- **Memory**: ~6GB VRAM
- **Speed**: ~30 samples/s
- **mAP**: 0.20-0.30 (tÃ¹y dataset)

## ğŸ“ Há»— trá»£

Náº¿u gáº·p váº¥n Ä‘á»:
1. Cháº¡y `quick_test.py` Ä‘á»ƒ kiá»ƒm tra
2. Kiá»ƒm tra logs trong `lightning_logs/`
3. Xem TensorBoard Ä‘á»ƒ monitor training
4. Thá»­ giáº£m batch size náº¿u out of memory

**ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸš€**
