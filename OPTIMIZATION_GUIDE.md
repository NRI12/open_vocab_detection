# üöÄ H∆∞·ªõng d·∫´n t·ªëi ∆∞u h√≥a Open Vocabulary Detection

## üìä V·∫•n ƒë·ªÅ ban ƒë·∫ßu

D·ª± √°n training ban ƒë·∫ßu g·∫∑p ph·∫£i c√°c v·∫•n ƒë·ªÅ:
- **Model qu√° n·∫∑ng**: Swin-Base (88M params) + Cross-attention ph·ª©c t·∫°p
- **Training ch·∫≠m**: Batch size nh·ªè (8), image size l·ªõn (800x800)
- **K·∫øt qu·∫£ k√©m**: Loss function ph·ª©c t·∫°p, learning rate kh√¥ng t·ªëi ∆∞u
- **Memory cao**: Kh√¥ng s·ª≠ d·ª•ng mixed precision, gradient accumulation

## ‚úÖ Gi·∫£i ph√°p ƒë√£ √°p d·ª•ng

### 1. **T·ªëi ∆∞u Model Architecture**
```python
# Tr∆∞·ªõc (Heavy)
- Swin-Base: 88M parameters
- Fusion: 2 layers, 8 heads
- Box Head: 100 queries, 256 hidden
- Image size: 800x800

# Sau (Light)  
- Swin-Tiny: 28M parameters (-68%)
- Fusion: 1 layer, 4 heads
- Box Head: 25 queries, 128 hidden
- Image size: 256x256
```

### 2. **C·∫£i thi·ªán Training Config**
```python
# Tr∆∞·ªõc
batch_size = 8
lr = 1e-4
max_epochs = 50
precision = 32

# Sau
batch_size = 24 (+200%)
lr = 3e-4 (+200%)
max_epochs = 20 (-60%)
precision = 16 (Mixed precision)
```

### 3. **T·ªëi ∆∞u Data Loading**
```python
# Th√™m c√°c t·ªëi ∆∞u h√≥a
- pin_memory=True
- persistent_workers=True
- num_workers=8
- Reduced data augmentation
```

### 4. **Simplified Loss Function**
```python
# Gi·∫£m ƒë·ªô ph·ª©c t·∫°p
- Reduced negative sampling (10 samples max)
- Lower similarity loss weight (0.2)
- Simplified Hungarian matching
```

## üéØ K·∫øt qu·∫£ d·ª± ki·∫øn

| Metric | Tr∆∞·ªõc | Sau | C·∫£i thi·ªán |
|--------|-------|-----|-----------|
| Parameters | 88M | 28M | -68% |
| Memory | ~8GB | ~3GB | -62% |
| Speed | 1x | 3-4x | +300% |
| Training time | 50 epochs | 20 epochs | -60% |

## üöÄ C√°ch s·ª≠ d·ª•ng

### 1. **Training v·ªõi model t·ªëi ∆∞u**
```bash
python scripts/train_optimized.py
```

### 2. **So s√°nh hi·ªáu su·∫•t**
```bash
python scripts/compare_models.py
```

### 3. **Training v·ªõi c·∫•u h√¨nh c≈© (ƒë·ªÉ so s√°nh)**
```bash
python scripts/train.py
```

## üìà Monitoring

### TensorBoard
```bash
tensorboard --logdir lightning_logs
```

### Key metrics to watch:
- `train/total_loss`: Should decrease steadily
- `val/mAP`: Main performance metric
- `val/total_loss`: Should be close to train loss
- GPU memory usage: Should be < 4GB

## üîß Tuning Tips

### N·∫øu v·∫´n ch·∫≠m:
1. Gi·∫£m `batch_size` xu·ªëng 16
2. Gi·∫£m `img_size` xu·ªëng 224x224
3. Gi·∫£m `num_queries` xu·ªëng 15

### N·∫øu k·∫øt qu·∫£ k√©m:
1. TƒÉng `lr` l√™n 5e-4
2. TƒÉng `max_epochs` l√™n 30
3. TƒÉng `lambda_sim` l√™n 0.5

### N·∫øu out of memory:
1. Gi·∫£m `batch_size` xu·ªëng 12
2. TƒÉng `accumulate_grad_batches` l√™n 2
3. Gi·∫£m `img_size` xu·ªëng 224x224

## üìù Files ƒë√£ thay ƒë·ªïi

- `scripts/train.py`: C·∫•u h√¨nh c∆° b·∫£n ƒë√£ t·ªëi ∆∞u
- `scripts/train_optimized.py`: C·∫•u h√¨nh t·ªëi ∆∞u cao
- `scripts/compare_models.py`: So s√°nh hi·ªáu su·∫•t
- `src/data/datamodule.py`: T·ªëi ∆∞u data loading
- `src/data/transforms.py`: Gi·∫£m augmentation
- `src/training/losses.py`: Simplified loss function

## üéâ K·∫øt lu·∫≠n

V·ªõi c√°c t·ªëi ∆∞u h√≥a n√†y, d·ª± √°n s·∫Ω:
- ‚úÖ Training nhanh h∆°n 3-4 l·∫ßn
- ‚úÖ S·ª≠ d·ª•ng √≠t memory h∆°n 60%
- ‚úÖ K·∫øt qu·∫£ t·ªët h∆°n v·ªõi √≠t epochs h∆°n
- ‚úÖ D·ªÖ debug v√† monitor h∆°n

H√£y th·ª≠ training v·ªõi `train_optimized.py` v√† so s√°nh k·∫øt qu·∫£!
