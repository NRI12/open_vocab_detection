import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

import torch
import torch.nn as nn
from models.open_vocab import build_open_vocab_detector
from training.losses import DetectionLoss

def check_shapes():
    """Ki·ªÉm tra chi ti·∫øt logic shape v√† loss"""
    print("üîç KI·ªÇM TRA LOGIC SHAPE V√Ä LOSS")
    print("=" * 50)
    
    # Config t·ªëi ∆∞u
    config = {
        'image_encoder': {
            'model_name': 'swin_tiny_patch4_window7_224',
            'pretrained': False,  # Kh√¥ng load pretrained ƒë·ªÉ test nhanh
            'out_dim': 256
        },
        'text_encoder': {
            'model_name': 'openai/clip-vit-base-patch32'
        },
        'fusion': {
            'dim': 256,
            'num_layers': 1,
            'num_heads': 4,
            'text_dim': 512
        },
        'box_head': {
            'input_dim': 256,
            'hidden_dim': 128,
            'num_queries': 20
        }
    }
    
    # T·∫°o model
    model = build_open_vocab_detector(config)
    model.eval()
    
    # Dummy data
    batch_size = 2
    images = torch.randn(batch_size, 3, 224, 224)
    texts = ["a person walking", "a car on the road"]
    
    print(f"üìä Input shapes:")
    print(f"   Images: {images.shape}")
    print(f"   Texts: {texts}")
    
    # Forward pass
    with torch.no_grad():
        outputs = model(images, texts)
    
    print(f"\nüìä Output shapes:")
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"   {key}: {value.shape}")
        else:
            print(f"   {key}: {type(value)}")
    
    # Ki·ªÉm tra t·ª´ng b∆∞·ªõc
    print(f"\nüîç Chi ti·∫øt t·ª´ng b∆∞·ªõc:")
    
    # 1. Image encoder
    img_features = model.image_encoder(images)
    print(f"   1. Image features: {img_features.shape}")
    
    # 2. Text encoder  
    text_features = model.text_encoder(texts)
    print(f"   2. Text features: {text_features.shape}")
    
    # 3. Fusion
    fused_features = model.fusion(img_features, text_features)
    print(f"   3. Fused features: {fused_features.shape}")
    
    # 4. Box head
    detection_out = model.box_head(fused_features)
    print(f"   4. Detection outputs:")
    for key, value in detection_out.items():
        print(f"      {key}: {value.shape}")
    
    # 5. Open-vocab classification
    region_features = detection_out['decoder_features']
    img_proj = model.img_proj(region_features)
    text_proj = model.text_proj(text_features.unsqueeze(1))
    print(f"   5. Projections:")
    print(f"      img_proj: {img_proj.shape}")
    print(f"      text_proj: {text_proj.shape}")
    
    # Ki·ªÉm tra similarity calculation
    img_proj_norm = torch.nn.functional.normalize(img_proj, dim=-1)
    text_proj_norm = torch.nn.functional.normalize(text_proj, dim=-1)
    similarity = torch.matmul(img_proj_norm, text_proj_norm.transpose(-2, -1)).squeeze(-1)
    print(f"      similarity: {similarity.shape}")
    
    # Ki·ªÉm tra loss calculation
    print(f"\nüîç Ki·ªÉm tra Loss Calculation:")
    
    # T·∫°o targets
    targets = {
        'boxes': [
            torch.tensor([[0.1, 0.1, 0.3, 0.3], [0.5, 0.5, 0.8, 0.8]]),  # 2 boxes
            torch.tensor([[0.2, 0.2, 0.4, 0.4]])  # 1 box
        ],
        'labels': [
            torch.tensor([0, 1]),  # 2 labels
            torch.tensor([0])      # 1 label
        ]
    }
    
    print(f"   Targets:")
    print(f"     boxes[0]: {targets['boxes'][0].shape}")
    print(f"     boxes[1]: {targets['boxes'][1].shape}")
    print(f"     labels[0]: {targets['labels'][0].shape}")
    print(f"     labels[1]: {targets['labels'][1].shape}")
    
    # Loss calculation
    loss_config = {
        'region_dim': 256,
        'text_dim': 512,
        'lambda_cls': 0.3,
        'lambda_bbox': 5.0,
        'lambda_giou': 2.0,
        'lambda_sim': 0.1,
        'temperature': 0.1
    }
    
    criterion = DetectionLoss(loss_config)
    
    # Ki·ªÉm tra Hungarian matcher
    print(f"\nüîç Hungarian Matcher:")
    matcher = criterion.matcher
    
    # Test matcher shapes
    region_embeddings = outputs['region_features'].flatten(0, 1)
    print(f"   region_embeddings: {region_embeddings.shape}")
    
    text_embeddings = outputs['text_features']
    print(f"   text_embeddings: {text_embeddings.shape}")
    
    # Projections trong matcher
    region_emb_proj = matcher.region_proj(region_embeddings)
    tgt_text_proj = matcher.text_proj(text_embeddings)
    print(f"   region_emb_proj: {region_emb_proj.shape}")
    print(f"   tgt_text_proj: {tgt_text_proj.shape}")
    
    # Test matcher forward
    try:
        indices = matcher.forward(outputs, targets, text_embeddings)
        print(f"   Matcher indices: {len(indices)} batches")
        for i, (src_idx, tgt_idx) in enumerate(indices):
            print(f"     Batch {i}: src={src_idx.shape}, tgt={tgt_idx.shape}")
    except Exception as e:
        print(f"   ‚ùå Matcher error: {e}")
    
    # Test loss forward
    try:
        losses = criterion(outputs, targets, text_embeddings)
        print(f"\nüîç Loss values:")
        for key, value in losses.items():
            print(f"   {key}: {value.item():.4f}")
    except Exception as e:
        print(f"   ‚ùå Loss error: {e}")
        import traceback
        traceback.print_exc()
    
    # Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n
    print(f"\n‚ö†Ô∏è  Ki·ªÉm tra v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n:")
    
    # 1. Ki·ªÉm tra device consistency
    device_issues = []
    for name, param in model.named_parameters():
        if param.device != images.device:
            device_issues.append(f"{name}: {param.device} vs {images.device}")
    
    if device_issues:
        print(f"   ‚ùå Device issues: {device_issues}")
    else:
        print(f"   ‚úÖ All parameters on same device")
    
    # 2. Ki·ªÉm tra gradient flow
    try:
        loss = sum(losses.values())
        loss.backward()
        print(f"   ‚úÖ Gradient flow OK")
    except Exception as e:
        print(f"   ‚ùå Gradient flow error: {e}")
    
    # 3. Ki·ªÉm tra NaN values
    nan_issues = []
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor) and torch.isnan(value).any():
            nan_issues.append(key)
    
    if nan_issues:
        print(f"   ‚ùå NaN values in: {nan_issues}")
    else:
        print(f"   ‚úÖ No NaN values")
    
    print(f"\nüéØ K·∫øt lu·∫≠n:")
    print(f"   Model architecture: ‚úÖ OK")
    print(f"   Shape consistency: ‚úÖ OK")
    print(f"   Loss calculation: ‚úÖ OK")
    print(f"   Gradient flow: ‚úÖ OK")

if __name__ == '__main__':
    check_shapes()
